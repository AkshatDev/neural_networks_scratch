{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Logistic_Regression_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Things to know before -\n",
        "\n",
        "This is Logistic Regression from scratch. This is done and implemented in a way that this can easily be extended to Neural Networks.\n",
        "\n",
        "For now, we can assume this to be a single Neuron with one output.\n",
        "\n",
        "Formulas -\n",
        "\n",
        "Compact form -\n",
        "\n",
        "z = w.T X + b -> a = sigmoid(z) -> Loss l(a,y) = - [y * log(a) + (1-y) * log(1-a)]\n",
        "\n",
        "Our goal - Find optimal w* and b* that minimizes the Loss.\n",
        "\n",
        "Disclamer- We can write the whole formula in one line,\n",
        "loss(w,b) =  -[ y * log(sigmoid(w.T X + b)) + (1-y) - log(1-sigmoid(w.T X+ b))]\n",
        "We can minimize this also, but it gets a bit messy.\n",
        "\n",
        "We are going to use the compact form to keep everything nice and clean.\n",
        "\n",
        "Our goal is to get dl/dz. We can achieve that by finding dl/da * da/dz (chain rule).\n",
        "\n",
        "dl/da= (a-y)/ a(1-a)\n",
        "\n",
        "da/dz= a * (1-a)\n",
        "\n",
        "=> dl/dz = a-y\n",
        "\n",
        "Now, to update w after each iteration, we need to find dl/dw.\n",
        "\n",
        "dl/dw = dl/dz * dz/dw\n",
        "\n",
        "dz/dw = X\n",
        "\n",
        "=> dl/dw= (a-y)*X\n",
        "\n",
        "similarly update for b- \n",
        "\n",
        "dl/db = dl/dz*dz/db\n",
        "\n",
        "dz/db= 1 \n",
        "\n",
        "dl/db = a-y\n",
        "\n",
        "\n",
        "\n",
        "***********************\n",
        "\n",
        "**final update for all w's, w= 1/n * learning_rate * (a-y) * X**\n",
        "\n",
        "**final update for all b's, w= 1/n * learning_rate * (a-y)**"
      ],
      "metadata": {
        "id": "8R11fENFBl8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2vyUZkm9sbF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as m\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data():\n",
        "  X,y=make_classification(n_samples=1000,n_features=10,random_state=42) # Generating random data\n",
        "  X=np.concatenate((X,np.ones(X.shape[0]).reshape(X.shape[0],1) ), axis=1) # Adding a column of 1's to account for the bias term.\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  lr=0.001\n",
        "  lamb=0.01\n",
        "  y_train=y_train.reshape(y_train.shape[0],1)\n",
        "  y_test=y_test.reshape(y_test.shape[0],1)\n",
        "  return X_train.T, X_test.T, y_train.T, y_test.T,lr,lamb"
      ],
      "metadata": {
        "id": "LKFrpt_g-Kez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 1000 samples with each sample having 10 features.\n",
        "\n",
        "Adding 1 to features to account for the bias term."
      ],
      "metadata": {
        "id": "jrfPVxp7-117"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  a=1 / ( 1 + np.exp(-(z) ) ) \n",
        "  return a"
      ],
      "metadata": {
        "id": "ZaedLyKR_bwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(w,X):\n",
        "  return sigmoid( w.T.dot(X) )"
      ],
      "metadata": {
        "id": "-Eili3FNonH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(X,a,y):\n",
        "  dw = np.dot(X,(a-y).T)\n",
        "  return dw"
      ],
      "metadata": {
        "id": "iCsKuXxKAWuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_params(X):\n",
        "  w=np.random.randn(X.shape[0],1)\n",
        "  return w"
      ],
      "metadata": {
        "id": "LSTsQC1NJvTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8l13X31Non-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y, y_hat):\n",
        "    loss = np.mean(-y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat)) + np.mean(lamb/(2) * (w[0:10]**2))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "zPtfkWJUAsQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w,X):\n",
        "  return sigmoid( w.T.dot(X) )"
      ],
      "metadata": {
        "id": "2zUufh2nLWPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "  X_train, X_test, y_train, y_test,lr,lamb=generate_data()\n",
        "  w=initialize_params(X_train)\n",
        "  lo=[]\n",
        "  accuracy=[]\n",
        "  for i in range(0,500):\n",
        "    predictions=[]\n",
        "    a=forward_prop(w,X_train)\n",
        "    dw=back_prop(X_train,a,y_train)\n",
        "    w[0:10]= ((w[0:10] - ( lr * (dw[0:10]) )   ) + lamb* w[0:10] )/X_train.shape[1]\n",
        "    w[10:11]=((w[10:11] - ( lr * (dw[10:11]) )   ) )/X_train.shape[1]\n",
        "    for i in a.T:\n",
        "      predictions.append(1 if i>=0.5 else 0)\n",
        "    lo.append(loss(y_train,a))\n",
        "    accuracy.append(m.accuracy_score(y_train.T,predictions))"
      ],
      "metadata": {
        "id": "_b3vIQvcUZAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=predict(w,X_test)\n",
        "predictions_test=[]\n",
        "for i in a.T:\n",
        "  predictions_test.append(1 if i>=0.5 else 0)\n",
        "print('acc on test set',m.accuracy_score(y_test.T,predictions_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCuUFjofLfWb",
        "outputId": "d2dd2ca2-7879-4838-f54a-e8495a04a7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc on test set 0.8454545454545455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf=LogisticRegression()\n",
        "clf.fit(X_train.T,y_train.T)\n",
        "preds=clf.predict(X_test.T)\n",
        "test=y_test.reshape(y_test.shape[1],)\n",
        "print('sklearn accuracy on test',m.accuracy_score(test,preds))"
      ],
      "metadata": {
        "id": "rob07zwYGIf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7948c05-6561-4c28-801c-f9ac102d1965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn accuracy on test 0.8454545454545455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    }
  ]
}